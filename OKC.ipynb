{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ATL = pd.read_csv('ATL.txt')\n",
    "BKN = pd.read_csv('BKN.txt')\n",
    "BOS = pd.read_csv('BOS.txt')\n",
    "CHI = pd.read_csv('CHI.txt')\n",
    "CHO = pd.read_csv('CHO.txt')\n",
    "CLE = pd.read_csv('CLE.txt')\n",
    "DAL = pd.read_csv('DAL.txt')\n",
    "DEN = pd.read_csv('DEN.txt')\n",
    "DET = pd.read_csv('DET.txt')\n",
    "GSW = pd.read_csv('GSW.txt')\n",
    "HOU = pd.read_csv('HOU.txt')\n",
    "IND = pd.read_csv('IND.txt')\n",
    "LAC = pd.read_csv('LAC.txt')\n",
    "LAL = pd.read_csv('LAL.txt')\n",
    "MEM = pd.read_csv('MEM.txt')\n",
    "MIA = pd.read_csv('MIA.txt')\n",
    "MIL = pd.read_csv('MIL.txt')\n",
    "MIN = pd.read_csv('MIN.txt')\n",
    "NOP = pd.read_csv('NOP.txt')\n",
    "NYK = pd.read_csv('NYK.txt')\n",
    "OKC = pd.read_csv('OKC.txt')\n",
    "ORL = pd.read_csv('ORL.txt')\n",
    "PHI = pd.read_csv('PHI.txt')\n",
    "PHO = pd.read_csv('PHO.txt')\n",
    "POR = pd.read_csv('POR.txt')\n",
    "SAC = pd.read_csv('SAC.txt')\n",
    "SAS = pd.read_csv('SAS.txt')\n",
    "TOR = pd.read_csv('TOR.txt')\n",
    "UTA = pd.read_csv('UTA.txt')\n",
    "WAS = pd.read_csv('WAS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_dict = {'Atlanta Hawks' : ATL,'Brooklyn Nets' : BKN,'Boston Celtics' : BOS,'Chicago Bulls' : CHI, 'Charlotte Hornets' : CHO,\n",
    "        'Cleveland Cavaliers' : CLE,'Dallas Mavericks' : DAL, 'Denver Nuggets' : DEN,'Detroit Pistons' : DET,'Golden State Warriors' : GSW, \n",
    "        'Houston Rockets' : HOU,'Indiana Pacers' : IND,'Los Angeles Clippers' : LAC, 'Los Angeles Lakers' : LAL,'Memphis Grizzlies' : MEM,\n",
    "        'Miami Heat' : MIA, 'Milwaukee Bucks' : MIL,'Minnesota Timberwolves' : MIN,'New Orleans Pelicans' : NOP, 'New York Knicks' : NYK,\n",
    "        'Oklahoma City Thunder' : OKC,'Orlando Magic' : ORL, 'Philadelphia 76ers' : PHI,'Phoenix Suns' : PHO,'Portland Trail Blazers' : POR, \n",
    "        'Sacramento Kings' : SAC,'San Antonio Spurs' : SAS,'Toronto Raptors' : TOR, 'Utah Jazz' : UTA,'Washington Wizards' : WAS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for team_name, team in t_dict.iteritems():\n",
    "        team['Away'] = np.where(team['Unnamed: 5']=='@',1,0)\n",
    "        team['Roadtrip'] = team['Away'] * (team['Away'].groupby((team['Away'] != team['Away'].shift()).cumsum()).cumcount() + 1)\n",
    "        team.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4','Unnamed: 5','Unnamed: 8','Notes'], axis=1, inplace = True)\n",
    "        team['diff'] = (team['Tm']-team['Opp'])\n",
    "        team['+/-'] = team['diff'].cumsum()\n",
    "        team['pct'] = np.round(team['W']/(team['W']+team['L']),3)\n",
    "        team['win'] = np.where(team['Unnamed: 7']=='W', 1, 0)\n",
    "        team['Res'] = team['Streak'].str[:1]\n",
    "        team['WS'] = np.where(team['Res']=='W', 1, 0)*team['Streak'].str[2:]\n",
    "        team['LS'] = np.where(team['Res']=='L', 1, 0)*team['Streak'].str[2:]\n",
    "        team['WStreak'] = np.where(team['WS']=='', 0,team['WS'])\n",
    "        team['LStreak'] = np.where(team['LS']=='', 0,team['LS'])\n",
    "        team['WStreak'] = pd.to_numeric(team['WStreak'])\n",
    "        team['LStreak'] = pd.to_numeric(team['LStreak'])\n",
    "        team.drop(['Res', 'WS', 'LS','diff', 'Unnamed: 7','Streak','Tm','Opp','W','L'], axis=1, inplace = True)\n",
    "        team['Prevdate'] = (pd.to_datetime(team['Date'])-pd.to_datetime(team['Date']).shift(+1)) #prev game in int\n",
    "        team.drop(team.index[:1], inplace=True)\n",
    "        team['Rest'] = (team['Prevdate']/ np.timedelta64(1, 'D')).astype(int)\n",
    "        team.drop(['Prevdate'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for team_name, team in t_dict.iteritems():\n",
    "    team[['Roadtrip','+/-','pct','win','WStreak','LStreak']] = \\\n",
    "    team[['Roadtrip','+/-','pct','win','WStreak','LStreak']].shift(+1)\n",
    "    team.drop(team.index[team['Date'].str.contains('Oct')], inplace=True)\n",
    "    team.drop(team.index[team['Date'].str.contains('Feb')], inplace=True)\n",
    "    team.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for team_name, team in t_dict.iteritems():\n",
    "    team['OppRoadtrip'] = np.nan\n",
    "    team['Opp+/-'] = np.nan\n",
    "    team['Opppct'] = np.nan\n",
    "    team['Oppwin'] = np.nan\n",
    "    team['OppWStreak'] = np.nan\n",
    "    team['OppLStreak'] = np.nan\n",
    "    team['OppRest'] = np.nan\n",
    "    for index, row in team.iterrows():\n",
    "        opp = row['Opponent']\n",
    "        opp_team = t_dict.get(opp)\n",
    "        date_comp = team.loc[index,'Date']\n",
    "        opp_index_list = opp_team.index[opp_team['Date'] == date_comp].tolist()\n",
    "        opp_index = 0\n",
    "        opp_index = opp_index_list.pop()\n",
    "        team.loc[index,'OppRoadtrip'] = opp_team.loc[opp_index,'Roadtrip']\n",
    "        team.loc[index,'Opp+/-'] = opp_team.loc[opp_index,'+/-']\n",
    "        team.loc[index,'Opppct'] = opp_team.loc[opp_index,'pct']\n",
    "        team.loc[index,'Oppwin'] = opp_team.loc[opp_index,'win']\n",
    "        team.loc[index,'OppWStreak'] = opp_team.loc[opp_index,'WStreak']\n",
    "        team.loc[index,'OppLStreak'] = opp_team.loc[opp_index,'LStreak']\n",
    "        team.loc[index,'OppRest'] = opp_team.loc[opp_index,'Rest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns = ['Away','Roadtrip','+/-','pct','win','WStreak','LStreak','Rest','OppRoadtrip',\\\n",
    "                              'Opp+/-','Opppct','Oppwin','OppWStreak','OppLStreak','OppRest'])\n",
    "for team_name, team in t_dict.iteritems():\n",
    "    team.drop(['G','Date','Opponent'], axis=1, inplace = True)\n",
    "    data = pd.concat([data, team], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###RUN TO OBTAIN LABELS###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ATL = pd.read_csv('ATL.txt')\n",
    "BKN = pd.read_csv('BKN.txt')\n",
    "BOS = pd.read_csv('BOS.txt')\n",
    "CHI = pd.read_csv('CHI.txt')\n",
    "CHO = pd.read_csv('CHO.txt')\n",
    "CLE = pd.read_csv('CLE.txt')\n",
    "DAL = pd.read_csv('DAL.txt')\n",
    "DEN = pd.read_csv('DEN.txt')\n",
    "DET = pd.read_csv('DET.txt')\n",
    "GSW = pd.read_csv('GSW.txt')\n",
    "HOU = pd.read_csv('HOU.txt')\n",
    "IND = pd.read_csv('IND.txt')\n",
    "LAC = pd.read_csv('LAC.txt')\n",
    "LAL = pd.read_csv('LAL.txt')\n",
    "MEM = pd.read_csv('MEM.txt')\n",
    "MIA = pd.read_csv('MIA.txt')\n",
    "MIL = pd.read_csv('MIL.txt')\n",
    "MIN = pd.read_csv('MIN.txt')\n",
    "NOP = pd.read_csv('NOP.txt')\n",
    "NYK = pd.read_csv('NYK.txt')\n",
    "OKC = pd.read_csv('OKC.txt')\n",
    "ORL = pd.read_csv('ORL.txt')\n",
    "PHI = pd.read_csv('PHI.txt')\n",
    "PHO = pd.read_csv('PHO.txt')\n",
    "POR = pd.read_csv('POR.txt')\n",
    "SAC = pd.read_csv('SAC.txt')\n",
    "SAS = pd.read_csv('SAS.txt')\n",
    "TOR = pd.read_csv('TOR.txt')\n",
    "UTA = pd.read_csv('UTA.txt')\n",
    "WAS = pd.read_csv('WAS.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#READ DATA AGAIN\n",
    "label_dict = {'Atlanta Hawks' : ATL,'Brooklyn Nets' : BKN,'Boston Celtics' : BOS,'Chicago Bulls' : CHI, 'Charlotte Hornets' : CHO,\n",
    "        'Cleveland Cavaliers' : CLE,'Dallas Mavericks' : DAL, 'Denver Nuggets' : DEN,'Detroit Pistons' : DET,'Golden State Warriors' : GSW, \n",
    "        'Houston Rockets' : HOU,'Indiana Pacers' : IND,'Los Angeles Clippers' : LAC, 'Los Angeles Lakers' : LAL,'Memphis Grizzlies' : MEM,\n",
    "        'Miami Heat' : MIA, 'Milwaukee Bucks' : MIL,'Minnesota Timberwolves' : MIN,'New Orleans Pelicans' : NOP, 'New York Knicks' : NYK,\n",
    "        'Oklahoma City Thunder' : OKC,'Orlando Magic' : ORL, 'Philadelphia 76ers' : PHI,'Phoenix Suns' : PHO,'Portland Trail Blazers' : POR, \n",
    "        'Sacramento Kings' : SAC,'San Antonio Spurs' : SAS,'Toronto Raptors' : TOR, 'Utah Jazz' : UTA,'Washington Wizards' : WAS}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for team_name, team in label_dict.iteritems():\n",
    "        team['Away'] = np.where(team['Unnamed: 5']=='@',1,0)\n",
    "        team['Roadtrip'] = team['Away'] * (team['Away'].groupby((team['Away'] != team['Away'].shift()).cumsum()).cumcount() + 1)\n",
    "        team.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4','Unnamed: 5','Unnamed: 8','Notes'], axis=1, inplace = True)\n",
    "        team['diff'] = (team['Tm']-team['Opp'])\n",
    "        team['+/-'] = team['diff'].cumsum()\n",
    "        team['pct'] = np.round(team['W']/(team['W']+team['L']),3)\n",
    "        team['win'] = np.where(team['Unnamed: 7']=='W', 1, 0)\n",
    "        team['Res'] = team['Streak'].str[:1]\n",
    "        team['WS'] = np.where(team['Res']=='W', 1, 0)*team['Streak'].str[2:]\n",
    "        team['LS'] = np.where(team['Res']=='L', 1, 0)*team['Streak'].str[2:]\n",
    "        team['WStreak'] = np.where(team['WS']=='', 0,team['WS'])\n",
    "        team['LStreak'] = np.where(team['LS']=='', 0,team['LS'])\n",
    "        team['WStreak'] = pd.to_numeric(team['WStreak'])\n",
    "        team['LStreak'] = pd.to_numeric(team['LStreak'])\n",
    "        team.drop(['Res', 'WS', 'LS','diff', 'Unnamed: 7','Streak','Tm','Opp','W','L'], axis=1, inplace = True)\n",
    "        team['Prevdate'] = (pd.to_datetime(team['Date'])-pd.to_datetime(team['Date']).shift(+1)) #prev game in int\n",
    "        team.drop(team.index[:1], inplace=True)\n",
    "        team['Rest'] = (team['Prevdate']/ np.timedelta64(1, 'D')).astype(int)\n",
    "        team.drop(['Prevdate'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for team_name, team in label_dict.iteritems():\n",
    "    team.drop(team.index[team['Date'].str.contains('Oct')], inplace=True)\n",
    "    team.drop(team.index[team['Date'].str.contains('Feb')], inplace=True)\n",
    "    team.drop(['G','Date', 'Opponent','Away', 'Roadtrip','+/-', 'pct','WStreak','LStreak','Rest'], axis=1, inplace = True)\n",
    "    team.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(columns = ['win'])\n",
    "for team_name, team in label_dict.iteritems():\n",
    "    labels = pd.concat([labels, team], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####GETTING DATA READY FOR MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data.values\n",
    "y = labels.values\n",
    "y=y.astype('int')\n",
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.asarray(X_scaled[:1000],dtype=np.float64)\n",
    "X_test = np.asarray(X_scaled[1000:],dtype=np.float64)\n",
    "y_train = np.asarray(y[:1000],dtype=np.float64)\n",
    "y_test = np.asarray(y[1000:],dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618251748252\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(activation='identity',alpha=0.001, #identiy seems to be working, large regularisation(gradient on full set)\n",
    "                    batch_size=X.shape[0], #small dataset, optimising on full\n",
    "                    hidden_layer_sizes=(30,50,30), #heuristically chosen\n",
    "                    learning_rate_init=0.001, max_iter=100, #small learning rate and many iterations(small dataset)\n",
    "                    solver='sgd') #stochastic grdient descent chosen(simplest method - no need for complexity)\n",
    "scores = cross_val_score(nn, X_scaled, y.ravel(), cv=10, scoring='accuracy') #10 fold - split ~ 1180/130\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.620454545455\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth=5, random_state=0) #don't allow trees to go deep, prevent overfitting\n",
    "scores = cross_val_score(rf, X_scaled, y.ravel(), cv=10, scoring='accuracy') #10 fold - split ~ 1180/130\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.522179487179\n"
     ]
    }
   ],
   "source": [
    "clf = linear_model.SGDClassifier(loss = 'squared_loss')\n",
    "scores = cross_val_score(clf, X_scaled, y.ravel(), cv=10, scoring='accuracy') #10 fold - split ~ 1180/130\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
